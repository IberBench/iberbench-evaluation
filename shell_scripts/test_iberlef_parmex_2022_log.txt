/data/research/sharedData/conda_envs/iborrego-lmeval-newnewtask/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `PYTORCH_PRETRAINED_BERT_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/research/sharedData/conda_envs/iborrego-lmeval-newnewtask/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `PYTORCH_TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/data/research/sharedData/conda_envs/iborrego-lmeval-newnewtask/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2025-02-04:17:05:24,070 INFO     [__main__.py:284] Verbosity set to INFO
2025-02-04:17:05:24,192 INFO     [__init__.py:459] The tag 'kobest' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.
2025-02-04:17:06:09,263 WARNING  [__main__.py:317]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-02-04:17:06:09,266 INFO     [__main__.py:369] Passed `--trust_remote_code`, setting environment variable `HF_DATASETS_TRUST_REMOTE_CODE=true`
2025-02-04:17:06:09,674 INFO     [__main__.py:381] Selected Tasks: ['iberlef_parmex_2022']
2025-02-04:17:06:09,708 INFO     [evaluator.py:165] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-02-04:17:06:09,861 INFO     [evaluator.py:202] Initializing hf model, with arguments: {'pretrained': 'EleutherAI/pythia-2.8b', 'trust_remote_code': True, 'parallelize': True, 'max_length': 2048}
2025-02-04:17:06:15,372 INFO     [huggingface.py:358] Model parallel was set to True, setting max memory per GPU to {0: 50759598080, 1: 50759598080} and device map to auto
The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48
2025-02-04:17:06:18,299 WARNING  [task.py:816] [Task: iberlef_parmex_2022] metric acc is defined, but higher_is_better is not. using default higher_is_better=True
2025-02-04:17:06:18,300 WARNING  [task.py:816] [Task: iberlef_parmex_2022] metric f1 is defined, but higher_is_better is not. using default higher_is_better=True
2025-02-04:17:06:21,908 INFO     [task.py:420] Building contexts for iberlef_parmex_2022 on rank 0...
  0%|          | 0/100 [00:00<?, ?it/s] 66%|██████▌   | 66/100 [00:00<00:00, 658.09it/s]100%|██████████| 100/100 [00:00<00:00, 234.47it/s]
2025-02-04:17:06:22,342 INFO     [evaluator.py:513] Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/200 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/200 [00:09<32:53,  9.92s/it]Running loglikelihood requests:  64%|██████▍   | 129/200 [00:10<00:03, 17.84it/s]Running loglikelihood requests: 100%|██████████| 200/200 [00:10<00:00, 19.58it/s]
2025-02-04:17:06:39,478 INFO     [evaluation_tracker.py:269] Output path not provided, skipping saving results aggregated
Passed argument batch_size = auto:16.0. Detecting largest batch size
Determined largest batch size: 64
Passed argument batch_size = auto:16.0. Detecting largest batch size
Determined largest batch size: 64
hf (pretrained=EleutherAI/pythia-2.8b,trust_remote_code=True,parallelize=True,max_length=2048,trust_remote_code=True), gen_kwargs: (None), limit: 1000.0, num_fewshot: None, batch_size: auto:16 (64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64,64)
|       Tasks       |Version|Filter|n-shot|Metric|   |Value|   |Stderr|
|-------------------|------:|------|-----:|------|---|----:|---|------|
|iberlef_parmex_2022|      1|none  |     0|acc   |↑  |0.230|±  |0.0423|
|                   |       |none  |     0|f1    |↑  |0.187|±  |   N/A|

